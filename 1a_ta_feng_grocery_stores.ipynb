{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1a. ta_feng grocery stores.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "NQuD_ylSyVqx",
        "cX0vJ5vDyZJw",
        "z9MoDfeyycJQ",
        "X1ShIL5dygEw",
        "UPoYgpIO-bmS",
        "c3_2aheH-fPX",
        "7mchMyVP-iFy",
        "MEBJ08Jh-sd3",
        "eEyt-MHh-vx2"
      ],
      "authorship_tag": "ABX9TyPugk+bj44SlBL/Fmx2/WB4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harnalashok/general/blob/master/1a_ta_feng_grocery_stores.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ucrjym_NsmZc"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Last amended: 25th August, 2022\n",
        "My folder: /home/ashok/Documents/ta_feng_grocerystore\n",
        "           C:\\Users\\ashok\\OneDrive\\Documents\\Ta Feng Grocery Datasets\n",
        "\n",
        "Ref:\n",
        "     https://pandas.pydata.org/pandas-docs/stable/cookbook.html#cookbook-grouping\n",
        "     https://pandas.pydata.org/pandas-docs/stable/groupby.html\n",
        "\n",
        "####################################################################3\n",
        "# Ta Feng Grocery dataset\n",
        "# Data Source: http://stackoverflow.com/questions/25014904/download-link-for-ta-feng-grocery-dataset\n",
        "#\tOther grocery datasets:\n",
        "#\t\thttps://sites.google.com/a/dlpage.phi-integration.com/pentaho/mondrian/mysql-foodmart-database/foodmart_mysql.tar.gz?attredirects=0\n",
        "#\t\thttp://recsyswiki.com/wiki/Grocery_shopping_datasets\n",
        "# References on clustering/customer segmentation or on kohonen SOM:\n",
        "# \t\t1. https://cran.r-project.org/web/views/Cluster.html\n",
        "#   \t2. http://www.shanelynn.ie/self-organising-maps-for-customer-segmentation-using-r/\n",
        "#\t\t3. General on kohonen: https://dzone.com/articles/self-organizing-maps\n",
        "#\t\t4. http://www.slideshare.net/jonsedar/customer-clustering-for-marketing\n",
        "#\n",
        "####################################################################\n",
        "\n",
        "# Objectives:\n",
        "            1. Reducing dataframe memory\n",
        "            2. Understand customer behaviour (using pandas groupby)\n",
        "            3. Chi-square Tests and mosaic plot\n",
        "            4. t-test of means\n",
        "            5. Feature Engineering\n",
        "\n",
        " ----Customer Behaviour----\n",
        " For  every customer (unique customerid)\n",
        "\t  Record his first purchase date\n",
        "\t  Record his last purchase date\n",
        "\t  Which customers purchase just one-time\n",
        "\t  Who are repeat purchasers\tor who have visited more than once\n",
        "\t  Record every customers total purchases\n",
        "\t  Record every customers average purchases\n",
        "\t  Record his basket of purchases: Variety of goods he purchases\n",
        "   Per visit/per transaction min. max items purchased and avg money spent\n",
        " For your store:\n",
        "\t  What is the distribution of customers age-wise?\n",
        "\t  What is the distribution of age, res-area-wise\n",
        "\t  Age wise what is the average purchase basket\n",
        "   Is there age preference for a particular product sub-class\n",
        " For a product-subclass\n",
        "\t  Which product-subclass brings most revenue\n",
        "   Which productids are most popular\n",
        "\tWhich productIds are most costly\n",
        "   And which customers purchase them?\n",
        " Tests:\n",
        "   IS there a relationship between age and product_subclass\n",
        "   IS there a relationship between residence_area and product_subclass\n",
        "   IS there a relationship between age and residence_area\n",
        "\n",
        "   Is there significant difference in avg spending, age-wise\n",
        "   Is there significant difference in avg spending, residence wise\n",
        "\n",
        " Questions not answered\n",
        "   Which days of week show heavy spending\n",
        "   Which days of month show heavy spending\n",
        "   Which days of week show least spending\n",
        "   which days of month show leat spending\n",
        "   What is customer life-time value\n",
        "   What are quarter-wise sales of product (use thicken())\n",
        "# *******************************\n",
        " ----Feature Engineering ----\n",
        "# *******************************\n",
        "  i)  Add a column to d12, that has counts of product_subclass\n",
        "      as many times as it occurs (FE1)\n",
        "  ii) Add a column to d12, that has counts of (age, residence_area)\n",
        "      as many times as the combination occurs (FE2)\n",
        " iii) Add a column that has variance of (age,residence_area) wise spending (FE3)\n",
        " iv)  Create a loop for the purpose (FE4)\n",
        "\n",
        "\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Call libraries"
      ],
      "metadata": {
        "id": "NQuD_ylSyVqx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Call libraries\n",
        "# 1.0 Reset memory\n",
        "#     ipython magic command\n",
        "#%reset -f\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# 1.1 For chi-square tests\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "# 1.2 For t-test\n",
        "from scipy.stats import ttest_ind\n",
        "# 1.3 Finding out score at a percentile point and\n",
        "#     pearson correlation coeff function\n",
        "from scipy.stats import scoreatpercentile, pearsonr\n",
        "\n",
        "\n",
        "# 1.4\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "# 1.4.1 Mosaic plots\n",
        "# https://www.statsmodels.org/dev/graphics.html\n",
        "# https://www.statsmodels.org/dev/generated/statsmodels.graphics.mosaicplot.mosaic.html#statsmodels.graphics.mosaicplot.mosaic\n",
        "from statsmodels.graphics.mosaicplot import mosaic\n",
        "\n",
        "\n",
        "# 1.5 Misc facilities\n",
        "from collections import Counter\n",
        "import os, time, sys, gc\n"
      ],
      "metadata": {
        "id": "OOJ2GVsptCtF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1.6 Print/display mutiple command outputs from a jupyter cell\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\""
      ],
      "metadata": {
        "id": "VY8L-8z6s7My"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mount gdrive"
      ],
      "metadata": {
        "id": "cX0vJ5vDyZJw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1.7 Mount gdrive\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "metadata": {
        "id": "qcRWg0hItyp6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.0 Set working folder and list files\n",
        "#\n",
        "pathToFolder = \"/gdrive/MyDrive/Colab_data_files/ta_feng_grocery store\"\n",
        "os.chdir(pathToFolder)\n",
        "os.listdir()\n"
      ],
      "metadata": {
        "id": "S5jvADQpw4Ci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define function to reduce mem usage"
      ],
      "metadata": {
        "id": "z9MoDfeyycJQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.1 This function reduces memory usage of a DataFrame\n",
        "# Ref: https://www.kaggle.com/c/champs-scalar-coupling/discussion/96655\n",
        "def reduce_mem_usage(df, verbose=True):\n",
        "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
        "    start_mem = df.memory_usage().sum() / 1024**2\n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtypes\n",
        "        if col_type in numerics:\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            if str(col_type)[:3] == 'int':\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)\n",
        "            else:\n",
        "                c_prec = df[col].apply(lambda x: np.finfo(x).precision).max()\n",
        "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max and c_prec == np.finfo(np.float16).precision:\n",
        "                    df[col] = df[col].astype(np.float16)\n",
        "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max and c_prec == np.finfo(np.float32).precision:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float64)\n",
        "    end_mem = df.memory_usage().sum() / 1024**2\n",
        "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
        "    return df"
      ],
      "metadata": {
        "id": "p8-fPXWOum5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read data & Reduce memory"
      ],
      "metadata": {
        "id": "X1ShIL5dygEw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.1 Read directly from zip file \n",
        "df = pd.read_csv(\"dall.csv.zip\", parse_dates = ['datetime'])"
      ],
      "metadata": {
        "id": "N3QVXFeeunc8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.1.1 Some exploration\n",
        "df.head()\n",
        "df.describe()"
      ],
      "metadata": {
        "id": "HTHEtGL-uRbI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.1.2 Examine data types now\n",
        "df.dtypes\n"
      ],
      "metadata": {
        "id": "x22XbZl1uw2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.1.3\n",
        "df = reduce_mem_usage(df)\n",
        "gc.collect()\n"
      ],
      "metadata": {
        "id": "W8mfgqjHuNoy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.1.4. We have no use for 'asset' column. Drop it\n",
        "\n",
        "df.drop(columns = ['asset'], inplace = True)\n"
      ],
      "metadata": {
        "id": "_85-5nkpy0jz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3.1\n",
        "# To further save memory, transform following five attributes\n",
        "#  to pandas 'category' type\n",
        "\n",
        "#  1. customerid --------int64,\n",
        "#  2. product_subclass---int64,\n",
        "#  3. residence_area-----object,\n",
        "#  4. age----------------object,\n",
        "#  5. productid----------int64\n",
        "\n",
        "\"Current memory usage: {} \".format(df.memory_usage().sum())"
      ],
      "metadata": {
        "id": "FYB3cBipy_BA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3.1.1 Transform now:\n",
        "\n",
        "df['customerid']       = df['customerid'].astype('category')\n",
        "df['product_subclass'] = df['product_subclass'].astype('category')\n",
        "df['residence_area']   = df['residence_area'].astype('category')\n",
        "df['age']              = df['age'].astype('category')\n",
        "df['productid']        = df['productid'].astype('category')"
      ],
      "metadata": {
        "id": "i9lxRJ0DvVHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3.1.2\n",
        "\n",
        "\"Present memory: {} \".format(df.memory_usage().sum())\n",
        "print()\n",
        "gc.collect()            # Release memory back to system"
      ],
      "metadata": {
        "id": "Cj4bWmHqwL9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3.2 Extract year, month, day`from 'datetime' field\n",
        "# Ref:https://pandas.pydata.org/pandas-docs/version/0.22/api.html#datetimelike-properties\n",
        "#     https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DatetimeIndex.html\n",
        "\n",
        "# 3.2.1 \n",
        "df['year']  =  df['datetime'].dt.year\n",
        "df['month'] =  df['datetime'].dt.month\n",
        "df['day']   =  df['datetime'].dt.day"
      ],
      "metadata": {
        "id": "P0CUPEixz6Os"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Start asking questions"
      ],
      "metadata": {
        "id": "hse5UD3V100o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Here is your data\n",
        "df.head(3)"
      ],
      "metadata": {
        "id": "5HmdMKYsDb4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q1\n",
        "How many unique customers, products & product_subclasses exist"
      ],
      "metadata": {
        "id": "UPoYgpIO-bmS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Answer"
      ],
      "metadata": {
        "id": "Q0jzos8hw7wR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4 (Q1) How many unique customers, productids & product_subclasses exist\n",
        "print(\"\\n Unique customer Ids--\")\n",
        "df['customerid'].nunique()                      # 32266\n",
        "\n",
        "\n",
        "print(\"\\n\\n==Unique products Ids==\")\n",
        "df['productid'].nunique()                       # 23812\n",
        "\n",
        "print(\"\\n\\n==Unique Product classes Ids==\")\n",
        "df['product_subclass'].nunique()                # 2012"
      ],
      "metadata": {
        "id": "HzFoP8Bu1bI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q2\n",
        "Who are the oldest customers?"
      ],
      "metadata": {
        "id": "c3_2aheH-fPX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Answer"
      ],
      "metadata": {
        "id": "fIXwBIz51Ws9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "####### Groupby:\n",
        "# 5.1 (Q2) \n",
        "#          Sol: Gr by customerid and for each\n",
        "#               customer find his min date. Then, sort it\n",
        "#               to find earliest purchase date of every customer\n",
        "\n",
        "# Show some groupby operations here\n",
        "\n",
        "grpd = df.groupby(['customerid'])\n",
        "grpd\n"
      ],
      "metadata": {
        "id": "YRWLX5-02Ad2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5.1.1 \n",
        "grpd['datetime'].min().sort_values(ascending = True)"
      ],
      "metadata": {
        "id": "slsBAHDt5XaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q3\n",
        "Recency: Find the last purchase date of every customer"
      ],
      "metadata": {
        "id": "7mchMyVP-iFy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Answer"
      ],
      "metadata": {
        "id": "9ASKRv7x1hhF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 5.2 (Q3) Sol:Group by customerid and find the last date of purchase\n",
        "\n",
        "result1 = df.groupby('customerid')['datetime'].max().sort_values(ascending = False)\n",
        "result1.head()\n",
        "result1.size    # 32266\n"
      ],
      "metadata": {
        "id": "xbrA5DHK4znn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q4\n",
        "Discover Repeat customers: Find repeat customers"
      ],
      "metadata": {
        "id": "MEBJ08Jh-sd3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Answer"
      ],
      "metadata": {
        "id": "kZoglXHo1__N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## 5.3 (Q4): \n",
        "##           Many ways to find repeat-customers\n",
        "##           a. Group customers by both cid and date and count\n",
        "##           b. Group the earlier table by cid and count\n",
        "##           c. Use apply with function\n",
        "##           d. Use apply with lambda"
      ],
      "metadata": {
        "id": "7LPNgfcU6nEf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5.4 Method 1: In each box how many unique datetime are there\n",
        "out = df.groupby(df['customerid'], as_index = False)\n",
        "out['datetime'].nunique()"
      ],
      "metadata": {
        "id": "VTZHTv7T6wyO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5.4.1 Check for one customerid\n",
        "df.loc[df['customerid'] == 1069]"
      ],
      "metadata": {
        "id": "dnh3kretn2SS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5.4.2 Exclude where visit is 1\n",
        "out[out > 1].sort_values(ascending = False)"
      ],
      "metadata": {
        "id": "ZKhizBAIlMBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#5.5 Use apply()\n",
        "# 5.5.1 First define a function:\n",
        "#       Function must return either a scalar or a pandas object\n",
        "#       Its argument is a dataframe\n",
        "\n",
        "def atx(fd):\n",
        "    return fd.datetime.nunique()\n",
        "\n",
        "\n",
        "# 5.5.2 First group by 'customerid'\n",
        "grouped = df.groupby('customerid')\n",
        "\n",
        "\n",
        "# 5.5.3 Extract a data subset\n",
        "#       Each subset is a dataframe\n",
        "r1 = grouped.get_group(1975543)\n",
        "r1                  # It is a dataframe\n",
        "type(r1)\n",
        "r2 = grouped.get_group(915939)\n",
        "r2                 # Another dataframe\n",
        "\n",
        "# 5.5.3 To test, apply the function\n",
        "#       to each subset\n",
        "atx(r1)\n",
        "atx(r2)\n",
        "\n",
        "\n",
        "# 5.5.4 Finally apply the function tx()\n",
        "#       ti each grouped-subset\n",
        "grouped.apply(atx).sort_values(ascending = False)\n",
        "\n",
        "# 5.6 Use lambda\n",
        "#     Each 'x' passed in lambda is a babay-dataframe\n",
        "#     And value returned is True/False\n",
        "grouped = df.groupby('customerid')\n",
        "\n",
        "result = grouped['datetime'].apply(lambda x : x.nunique() > 1).sort_values()\n",
        "result.head()\n",
        "\n"
      ],
      "metadata": {
        "id": "nN899lzK7OVH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q5\n",
        " What are total no of visits of a customer during the period of data?"
      ],
      "metadata": {
        "id": "eEyt-MHh-vx2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Answer"
      ],
      "metadata": {
        "id": "7_LoOGkL2MSW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 6.  (Q5): What is total no of visits of a customer during the period of data\n",
        "#     Sol:Group by customerid, find distinct dates & count them\n",
        "\"\"\"\n",
        "# Which aggregating functions will work?\n",
        "   The aggregating functions above will exclude NA values.\n",
        "   Any function which reduces a Series to a scalar value is\n",
        "   an aggregation function and will work, a trivial example\n",
        "   is df.groupby('A').agg(lambda ser: 1).\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "out = df.groupby(df['customerid'])['datetime'].nunique().sort_values(ascending=False)\n",
        "out.head()\n",
        "out.tail()"
      ],
      "metadata": {
        "id": "EyXCBzzO7cZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6.1 Check for a customerid:\n",
        "\n",
        "out.loc[1069]"
      ],
      "metadata": {
        "id": "i3CCmtmhpW6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6.2 Let us see distribution of visit-frequencies\n",
        "# 6.2.1  First convert out (pandas Series) to DataFrame\n",
        "\n",
        "out.name           # This series name will become column name\n",
        "\n"
      ],
      "metadata": {
        "id": "WLVUCL477zhb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6.2.2 Assign a name to the Series:\n",
        "\n",
        "out.name = \"freq\""
      ],
      "metadata": {
        "id": "roJdG4OWqIMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6.2.3 Transform the Series to DataFrame\n",
        "#      Series name becomes its column name\n",
        "\n",
        "out = pd.DataFrame(out)   # column name is now 'freq'"
      ],
      "metadata": {
        "id": "4UOJRy2jqCXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6.2.4 Draw boxplot now uisng pandas plot function:\n",
        "\n",
        "out.boxplot(column = 'freq')\n",
        "plt.ylim((0,20))\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "r2lfDzQ3p810"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6.3 Let us verify the above results for at least one customerid:\n",
        "\n",
        "df.loc[df['customerid'] == 1847994 ].nunique()"
      ],
      "metadata": {
        "id": "L4KtPbll_CnG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6.3.1 Box plots with pandas:\n",
        "\n",
        "data = np.random.randn(25, 4)\n",
        "dfx = pd.DataFrame(data, columns=list('ABCD'))\n",
        "dfx.plot.box();"
      ],
      "metadata": {
        "id": "M8i3yBIl8DAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q6\n",
        "What are total spending per customer?"
      ],
      "metadata": {
        "id": "vwCUYKsW_12v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 7 (Q6): \n",
        "#    Sol: Group by customerid and sum up purchases\n",
        "\n",
        "# 7.1 First create a new column 'purchase'\n",
        "df['purchase'] = df['quantity'] * df['salesprice']\n",
        "\n",
        "# 7.1.1 Now calculate total purchases per customer\n",
        "out = df.groupby(['customerid'])['purchase'].sum().sort_values(ascending = False)\n",
        "\n",
        "# 7.1.2 out is a pandas Series\n",
        "out.head()\n",
        "\n",
        "# 7.1.3 Make out a DataFrame\n",
        "out = pd.DataFrame(out)"
      ],
      "metadata": {
        "id": "eV_cm3y18-dW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7.2 Let us have a density plot of these purchases:\n",
        "\n",
        "plt.figure() ;\n",
        "out.plot.kde()     # Pandas kernel denity plot\n",
        "plt.xlim(-0.2e8, .1e8) ;\n",
        "plt.show() ;"
      ],
      "metadata": {
        "id": "up22dTs2_6Fw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7.2.1 seaborn does not plot that well:\n",
        "\n",
        "out1 = out.sample(n = 5000)\n",
        "sns.histplot(data = out1, x = 'purchase')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-_P7MmBRAWAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7.2.2 So there is an outlier, let us remove\n",
        "#       it and then plot. We will cutoff at 99th percentile\n",
        "#       At what value of data 99th percentile occurs?\n",
        "\n",
        "scoreatpercentile(out.values, per = 99)\n",
        "\n",
        "# 7.2.3 Here is remaining data\n",
        "\n",
        "out = out.loc[out['purchase'] < 52135, :]\n",
        "\n",
        "\n",
        "# 7.2.4 Plot density plot now\n",
        "plt.figure() ;\n",
        "out.plot.kde() ;\n",
        "plt.show() ;"
      ],
      "metadata": {
        "id": "ruS6V6ruAmfv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q7\n",
        "Is there any relationship between freq of visits and total purchases?"
      ],
      "metadata": {
        "id": "8pltoM4OK0xm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Answer"
      ],
      "metadata": {
        "id": "MuJY5WVe25h2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Is there any relationship between freq of visits and total purchases\n",
        "#    We will use two methods:\n",
        "\n",
        "## Method 1\n",
        "\n",
        "# 8.1   First customer-wise visit frequencies:\n",
        "\n",
        "freq = df.groupby('customerid')['datetime'].nunique()\n",
        "freq.name = \"freq\"\n",
        "freq.head()"
      ],
      "metadata": {
        "id": "7yptXFVCK6M7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8.2 Then customer-wise purchases:\n",
        "\n",
        "purchases = df.groupby('customerid')['purchase'].sum()\n",
        "purchases"
      ],
      "metadata": {
        "id": "v1N4iAlxK-i8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8.3 Create a dataframe from two continuous series:\n",
        "\n",
        "freq_purchases = pd.concat([freq,purchases], axis = 1)\n",
        "freq_purchases.head()"
      ],
      "metadata": {
        "id": "pwiYXE97LELX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8.4 So what kind of relationships exist?\n",
        "#     Answer: Very weak!\n",
        "\n",
        "freq_purchases.corr()"
      ],
      "metadata": {
        "id": "XbrcUFxELd98"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8.5 Include only points which are below 99th percentile of purchases\n",
        "\n",
        "freq_purchases = freq_purchases[freq_purchases['purchase'] < 52135]"
      ],
      "metadata": {
        "id": "k2R8h0eNLsG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8.6 Let us revisit relationships\n",
        "#     Relationship is stronger though not very strong\n",
        "\n",
        "freq_purchases.corr()"
      ],
      "metadata": {
        "id": "PXK1uobPLvra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8.7 Plot and also annotate now\n",
        "#     First through pandas plotting methods\n",
        "plt.figure()\n",
        "freq_purchases.plot.scatter(x = 'freq', y = 'purchase')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7BwQNMaNICTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q8\n",
        "What are average purchases per-customer, per visit?"
      ],
      "metadata": {
        "id": "_48R9e2NNUu0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Answer"
      ],
      "metadata": {
        "id": "3hD6rfEF3E5X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. (Q8):  What are average purchases per-customer, per visit\n",
        "#     Sol: Simple: Gr by customerid,datetime and sum up purchases\n",
        "#\n",
        "result = df.groupby(['customerid', 'datetime'])['purchase'].sum()\n",
        "result.head()"
      ],
      "metadata": {
        "id": "x77u5SqSNcMY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 9.1 Note that grouping by more than one attribute\n",
        "#     creates multiindex. Here it has two levels\n",
        "\n",
        "result.index.get_level_values(0)\n",
        "result.index.get_level_values(1)"
      ],
      "metadata": {
        "id": "SMZXV3PeNjlX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 9.2 Unentangle it using unstack()\n",
        "result1 = df.groupby(['customerid', 'datetime'])['purchase'].sum().unstack()\n",
        "result1.head()"
      ],
      "metadata": {
        "id": "ZHz-EEttNqkt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 9.3 Find mean, across columns\n",
        "#     And this our answer to the question\n",
        "df.groupby(['customerid', 'datetime'])['purchase'].sum().unstack().mean(axis = 1, skipna = True).head()"
      ],
      "metadata": {
        "id": "OrMx64mNN0-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 9.4 Let us verify the result for at least one customer\n",
        "df[df['customerid'] == 1069].groupby('datetime')['purchase'].sum()\n",
        "(187 + 971 + 922 + 580)/4               # 665\n"
      ],
      "metadata": {
        "id": "2xNJFNF6NV6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q9"
      ],
      "metadata": {
        "id": "Owve-b0nOMOt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " What are average purchases customer-wise?"
      ],
      "metadata": {
        "id": "rSZvACR93L81"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Answer"
      ],
      "metadata": {
        "id": "8vPKtX6j3Nhs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 10 (Q9): \n",
        "#          Sol: Gr by customerid and find mean() purchases per id\n",
        "\n",
        "df.groupby('customerid')['purchase'].mean().sort_values(ascending = False)\n"
      ],
      "metadata": {
        "id": "DPFe3lFQOEa0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q10\n",
        "Determine customer-wise, product_subclass preference?"
      ],
      "metadata": {
        "id": "GXw5OKCHOXEM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Answer"
      ],
      "metadata": {
        "id": "-o5RgnoH3R3y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## 11. Q9\n",
        "# 11.1 \n",
        "#     We will define preference as no of different datetimes purchased.\n",
        "#     All purchases of a product_subclass on one date count to one.\n",
        "\n",
        "result = df.groupby(['customerid', 'product_subclass'])['datetime'].nunique().sort_values(ascending= False)\n",
        "result.head()\n",
        "\n",
        "# 11.2 Just to extract result for one customer from this multiindex, use\n",
        "#      index.get_level_values()....rather complicated but works\n",
        "#  Ref: https://stackoverflow.com/questions/17921010/how-to-query-multiindex-index-columns-values-in-pandas\n",
        "result.loc[result.index.get_level_values('customerid')  == 1740653, :]\n",
        "\n",
        "\n",
        "# 11.3 But if preference means by quantity, then solution is:\n",
        "df.groupby(['customerid', 'product_subclass'])['quantity'].sum().sort_values(ascending= False)"
      ],
      "metadata": {
        "id": "CMioD0UhOQDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q11\n",
        "Customer-wise basket of purchases"
      ],
      "metadata": {
        "id": "Is3dCoSoOgFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Answer"
      ],
      "metadata": {
        "id": "Qrwu_tTf3c6G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 12 (Q10): Basket of purchases:\n",
        "#            Variety of purchases made per customer, Productid wise\n",
        "\n",
        "df.groupby(['customerid'])['productid'].nunique().sort_values(ascending = False)\n"
      ],
      "metadata": {
        "id": "dylYh0kHOhUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q12\n",
        "Which product_class brings most revenue?"
      ],
      "metadata": {
        "id": "d6WgtXIsOrRs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Answer"
      ],
      "metadata": {
        "id": "TbVsKBe93hLU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 13  Which product_class brings most revenue\n",
        "#        Sol: Group by product_subclass and add quantity * salesprice\n",
        "\n",
        "# 13.1 Here is the solution:\n",
        "df.groupby(['product_subclass'])['purchase'].sum()\n"
      ],
      "metadata": {
        "id": "f0XlK9eMOiIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q13\n",
        "Which product_subclass is popular that is         most customers buy?"
      ],
      "metadata": {
        "id": "7dDVXIASO22l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Answer"
      ],
      "metadata": {
        "id": "Bq52Q9Gj3yFV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 13.2 (Q12): Which product_subclass is popular that is\n",
        "#             most customers buy?\n",
        "df.groupby(['product_subclass'])['customerid'].nunique().sort_values(ascending=False)\n"
      ],
      "metadata": {
        "id": "A48b9a37Owyg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise questions\n"
      ],
      "metadata": {
        "id": "SOdXMTE9O-hk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Not solved. Students to solve these\n",
        "# 13.3 (Q14): Age wise purchases average. Which age group max purchases\n",
        "#             Just group by age\n",
        "\n",
        "# 13.4 (Q15): Residence area wise purchasing capacity\n",
        "#             Just group by residence area wise\n",
        "\n",
        "\n",
        "# 13.5 (Q16)  Per visit/per transaction avg money spent\n",
        "\n",
        "\n",
        "# 14   (Q17): Distribuiton of age groups with residence_area\n",
        "#             Same as:  table(d12$age, d12$residence_area)\n",
        "\n",
        "# 15.  (Q18)    What is the distribution of customers, age-wise\n"
      ],
      "metadata": {
        "id": "qfjB51PKO5Wn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q14\n",
        "Is there a relationship between 'age' and 'residence_area'?"
      ],
      "metadata": {
        "id": "_YXPhorpPgVs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Answer"
      ],
      "metadata": {
        "id": "svGM6ecC34u1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 16. (Q18) Is there a relationship between 'age' and 'residence_area'\n",
        "#           We will calculate ch-square statistics\n",
        "#           An often quoted guideline for the validity of chi-square calculation\n",
        "#           is that the test should be used only if the observed and expected\n",
        "#           frequencies in each cell are at least 5.\n",
        "\n",
        "# 16.1 Return values are: chi2 statistic, p-value, degrees-of-freedom, expected-freq\n",
        "chi2_contingency(pd.crosstab(df.age, df.residence_area))     # p-value = 0\n",
        "# OR\n",
        "# 16.2\n",
        "chi2, p_value, dof, expeFreq = chi2_contingency(pd.crosstab(df.age, df.residence_area))\n",
        "p_value\n",
        "\n",
        "\n",
        "\n",
        "# 17. Mosiac plot of contingency table\n",
        "#     At a glance view of deviation from expected freq\n",
        "# statistic=True  will give colors to the plot. If the tile has a freq\n",
        "#   is more than 2 standard deviation from the expected value\n",
        "#    color will go from green to red (for positive deviations, blue otherwise)\n",
        "#     and will acquire an hatching when crosses the 3 sigma.\n",
        "fig = plt.figure(figsize = (10,10))     # Set figure size\n",
        "ax = fig.add_subplot(111)               # Add one subplot\n",
        "mosaic(df, ['age', 'residence_area'],\n",
        "       ax = ax,\n",
        "       statistic = True\n",
        "       )\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# 18   t-test IS there any significant difference in avg spending\n",
        "#        age-wise, say, between ages 'A' and 'B'\n",
        "#   Steps:\n",
        "#        1. Extract spending data for two ages\n",
        "#        2. Discover 99th percentile points for each\n",
        "#        3. Remove outliers\n",
        "#        4. Perform t-test\n",
        "\n",
        "# 18.1 'purchase' data for age = 'A'\n",
        "a = df.loc[df['age'] == 'A', 'purchase']\n",
        "len(a)              # 30068\n",
        "\n",
        "# 18.2 'purchase' data for age = 'B'\n",
        "b = df.loc[df['age'] == 'B', 'purchase']\n",
        "len(b)              # 66427\n",
        "\n",
        "# 18.3 Conduct t-test now. Are the means of two purchases equal?\n",
        "_, pvalue = ttest_ind( a, b, axis=0)\n",
        "pvalue      # 0.18781 So no difference in purchase mean\n",
        "\n",
        "# 18.4 Let us remove few outliers and then again perform t-test\n",
        "#      What is the 99th percentile in each case\n",
        "scoreatpercentile(a, per = 99)             # 2558\n",
        "scoreatpercentile(b, per = 99)             # 3029\n",
        "\n",
        "# 18.5 Extract from 'a' & 'b' values other than outlers\n",
        "a= a[a<2558]\n",
        "b = b[b<3029]\n",
        "len(a)        # 29767\n",
        "len(b)        # 65762\n",
        "\n",
        "# 18.6 Perform t-test now:\n",
        "# 18.7 Removing outliers drastically alters the conclusion\n",
        "_, pvalue = ttest_ind( a, b, axis=0)\n",
        "pvalue             # 3.060e-42\n",
        "\n"
      ],
      "metadata": {
        "id": "yZ2o35_9PCPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finish\n",
        " ---------------------I am done------------------------------------"
      ],
      "metadata": {
        "id": "3EtZt0z-39UU"
      }
    }
  ]
}