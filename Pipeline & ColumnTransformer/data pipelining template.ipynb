{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last amended:  10th July, 2020\n",
    "# My folder:    /home/ashok/Documents/5.decisiontree\n",
    "# VM: lubuntu_machinelearning_I\n",
    "# Ref Why dummy encoding:\n",
    "#        https://www.statisticssolutions.com/dummy-coding-the-how-and-why/\n",
    "\n",
    "# Objectives:\n",
    "#     i)    Read and explore data\n",
    "#    ii)    Deal with missing values \n",
    "#   iii)    OneHotEncode categorical features\n",
    "#   iv)     Use Pipeline and ColumnTransformer \n",
    "#            for data transformation\n",
    "#    v)     Pipeline for modeling\n",
    "#    vi)    Nested pipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.0 Reset memory\n",
    "%reset -f\n",
    "# 1.1 Call libraries\n",
    "\n",
    "## Data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 1.2 for data splitting\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "## Transformers:\n",
    "# 1.3 Class for imputing missing values\n",
    "from sklearn.impute import SimpleImputer\n",
    "# 1.4 One hot encode categorical data--Convert to dummy\n",
    "from sklearn.preprocessing import OneHotEncoder as onehot\n",
    "# 1.5 Scale numeric data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# 1.6 Label encode target column\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "## Composite Transformers\n",
    "# 1.7 Class for applying multiple data transformation jobs\n",
    "from sklearn.compose import ColumnTransformer\n",
    "# 1.8 Pipeline class\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# 1.9 Estimator\n",
    "# Ref: https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
    "# User guide: https://scikit-learn.org/stable/modules/tree.html\n",
    "from sklearn.tree import DecisionTreeClassifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.10 Display outputs of all commands from a cell--not just of the last command\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.0 Import warnings module\n",
    "import warnings\n",
    "# 2.1 Do not print warnings on screen\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.0 Create a toy dataset with six columns\n",
    "#     Every column has at least one NaN except column: 'creditability'\n",
    "#     'creditability' is our target column so no NaN\n",
    "df = pd.DataFrame({\n",
    "                    'creditability' : ['yes','yes','yes','yes','yes','yes','no','no','no','no','no','no','no','no'], # Target column\n",
    "                    'acc_balance'   : [1,2,1,np.nan,1,2,1,2,1,2,2,np.nan,np.nan,np.nan],\n",
    "                    'house_owned'   : ['big','small',np.nan,'small','big',np.nan,np.nan,'big','small','big','big','small',np.nan,'small'],\n",
    "                    'age'           : [21,45,np.nan,40,34,89,23,65,87,np.nan,90,np.nan,60,np.nan],\n",
    "                    'income'        : [np.nan,7.8,3.4,5.5,2.1,8.9,3.9,np.nan,6.9,9.0,np.nan,8.0,8.5,np.nan],  \n",
    "                    'credit_amount' : [1011,np.nan,3211,np.nan,1000,2323,1010,1500,1300,1782,1212,np.nan,1232,np.nan]\n",
    "                  }\n",
    "               )\n",
    "\n",
    "df    # (14,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 Engineer some new categorical features from 'age' and 'credit_amount'\n",
    "#     We will have NaN values both in 'age_cat' and 'credit_amount_cat' columns\n",
    "#     (Note: Strictly speaking this method of creating features outside pipeline is \n",
    "#     not recommended as it leaks information about X_test (to be created, see below)\n",
    "#     to X_train. Recommended way is to wrap it up inside sklearn's FunctionTransformer\n",
    "#     and then use wrapped transformer within a processing pipeline. See:\n",
    "#     https://scikit-learn.org/stable/modules/preprocessing.html#custom-transformers)\n",
    "#     Else, use KBinsDiscretizer of sklearn\n",
    "\n",
    "df['age_cat'] = pd.cut(df['age'],               # Equal interval cuts between min and max\n",
    "                       3,                       # Three cuts\n",
    "                       labels=[\"1\",\"2\", \"3\"]    # Label for each cut\n",
    "                       )\n",
    "\n",
    "# 3.1.1\n",
    "df['credit_amount_cat'] = pd.qcut(df['credit_amount'],       # Equal freq cut\n",
    "                                  3,\n",
    "                                  labels=[\"low\",\"medium\", \"high\"])\n",
    "df   # (13,8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 Randomly shuffle data as values \n",
    "#     in 'credibility' column have an order\n",
    "df = df.sample(frac = 1) \n",
    "df    # (13,8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.3    Popout target\n",
    "#        to separate predictors and target\n",
    "\n",
    "y = df.pop('creditability')\n",
    "y[:3]      # Pandas Series\n",
    "\n",
    "# 3.4   Create an alias of predictors dataset \n",
    "X = df     # X is another name for df\n",
    "X.shape    # (13,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.0 Split dataset. We will preprocess X_train and apply that\n",
    "#     processing to X_test later\n",
    "X_train,X_test, y_train, y_test = train_test_split(\n",
    "                                                    X,                   # Data features\n",
    "                                                    y,                   # Target column\n",
    "                                                    test_size = 0.3      # split-ratio\n",
    "                                                    )\n",
    "\n",
    "# 4.1 Note the use of f-string for printing\n",
    "f\"X_train shape: {X_train.shape}\"    # (9,7)\n",
    "f\"X_test.shape : {X_test.shape}\"     # (4,7)\n",
    "f\"y_train shape: {y_train.shape}\"    # (9,)\n",
    "f\"y_test shape : {y_test.shape}\"     # (4,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make copy of data set for two separate ways processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2   Make a copy of X_train\n",
    "#       and X_test for two separate\n",
    "#       ways of data processing\n",
    "#       without using pipes and with pipes\n",
    "\n",
    "X_train_c = X_train.copy()\n",
    "X_test_c  = X_test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate out categorical and numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4.3\n",
    "###    Which columns are categorical\n",
    "###    but disguised as integers\n",
    "\n",
    "# 4.3 How many unique values per column.\n",
    "#     Check every column\n",
    "#     We will assume that if number of unique values\n",
    "#      are 4 or less it is categorical column else numeric\n",
    "\n",
    "f\"Total no of unique values per column are:\"\n",
    "X_train_c.nunique()        # Total no of unique values in each column\n",
    "\n",
    "# 4.4 If no. of unique values less than 5, it is categorical\n",
    "f\"True are categorical and False are numerical:\"\n",
    "X_train_c.nunique() < 5    # All True are categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.5 Extract list of cat_cols and num_cols:\n",
    "\n",
    "# 4.6 First note which are cat and which are num\n",
    "dg = (X_train_c.nunique() < 5)  \n",
    "dg    # All True are cat and all False are num\n",
    "\n",
    "# 4.7 Then filter out names from Series index \n",
    "cat_cols = dg[dg==True].index.tolist()\n",
    "num_cols = dg[dg==False].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.8 Here are the columns\n",
    "cat_cols    #  4\n",
    "num_cols    #  3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.9 We will create two subsets of num_cols\n",
    "#      One set we will impute using 'mean' \n",
    "#       and the other using 'median'\n",
    "num_cols_mean   = ['age']\n",
    "num_cols_median = ['income', 'credit_amount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.10 We will create two sets of cat_cols\n",
    "#      One set we will fill with 'most_frequent'\n",
    "#       and the other using a constant value\n",
    "\n",
    "cat_cols_mf       = ['acc_balance', 'house_owned']       # 'most_frequent' fill\n",
    "cat_cols_constant = ['age_cat', 'credit_amount_cat']     # 'constant' fill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.11 So we have four datasets for imputing: These are:\n",
    "X_train[num_cols_mean]              # Num dataset, impute by 'mean'   strategy\n",
    "X_train[num_cols_median]            # Num dataset, impute by 'median' strategy\n",
    "X_train[cat_cols_mf]                # Cat dataset, impute by 'most_frequent' strategy\n",
    "X_train[cat_cols_constant]          # Cat dataset, impute by 'constant' strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I\n",
    "## Data preprocessing without Pipelining\n",
    "Pre-process each one of the four subsets of data separately. And finally manually concatenate all results to create final dataset. We will henceforth use two terms:<br>\n",
    "<ul>\n",
    "    <li>\n",
    "    transformers: Which transform a dataset. Examples: <i>StandardScaler()</i>, <i>Normalizer()</i>, <i>SimpleImputer()</i>,  <i>OneHotEncoder()</i>, <i>PipeLine()</i>, <i>ColumnTransformer()</i>. Transformers have <i>fit()</i>, <i>transform()</i> and <i>fit_transform()</i> methods. Transformers do not make any predictions and hence there is no <i>predict()</i> method.\n",
    "    </li>\n",
    "    <li>\n",
    "estimators:   Which estimate the pattern in a data. Example: <i>DecisionTreeClassifier</i>, <i>KMeans</i>, <i>GMM</i>.      Estimators have <i>fit()</i> and <i>predict()</i> methods. There is no need for any transformation and hence there is no <i>transform()</i> method.\n",
    "</ul></li>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-process the two subsets of numerical columns first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 5.1 Impute NaN first in 'num_cols_median'\n",
    "#      Median imputer\n",
    "\n",
    "# 5.1.1 Instantiate SimpleImputer object\n",
    "#        Note the strategy of filling NaN\n",
    "\n",
    "si_median = SimpleImputer(strategy = 'median')\n",
    "\n",
    "# 5.1.2 Next, use 'si_median' object to fit \n",
    "#       and transform at one go and overwrite \n",
    "#       our data-subset\n",
    "\n",
    "X_train_c[num_cols_median] = si_median.fit_transform(X_train_c[num_cols_median])\n",
    "\n",
    "# 5.1.3 Observe result. It should have no NaNs\n",
    "X_train_c[num_cols_median]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2 Mean imputer: Same steps as above but\n",
    "#     on different data-subset\n",
    "\n",
    "si_mean = SimpleImputer(strategy = 'mean')\n",
    "X_train_c[num_cols_mean] = si_mean.fit_transform(X_train_c[num_cols_mean])\n",
    "X_train_c[num_cols_mean]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-process two subsets of categorical columns, next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.3 Next impute subset of categorical columns\n",
    "#     with most_frequent\n",
    "\n",
    "si_mf = SimpleImputer(strategy = 'most_frequent')\n",
    "X_train_c[cat_cols_mf] = si_mf.fit_transform(X_train_c[cat_cols_mf])\n",
    "X_train_c[cat_cols_mf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.4 Next impute subset of categorical columns\n",
    "#     with constant value. Our constant value= 'missing'\n",
    "\n",
    "si_constant = SimpleImputer(strategy = 'constant', fill_value = 'missing')\n",
    "X_train_c[cat_cols_constant] = si_constant.fit_transform(X_train_c[cat_cols_constant])\n",
    "X_train_c[cat_cols_constant]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## A Summary\n",
    "# 5.5 So our fitted transformers are:\n",
    "\n",
    "si_median         # To transform num_cols_median\n",
    "si_mean           # To transform num_cols_mean\n",
    "si_mf             # To transform cat_cols_mf\n",
    "si_constant       # To transform cat_cols_constant\n",
    "\n",
    "# 5.6 Our column-colections are:\n",
    "\n",
    "cat_cols          # cat_cols_mf + cat_cols_constant\n",
    "num_cols          # num_cols_mean + num_cols_median\n",
    "\n",
    "cat_cols_mf\n",
    "cat_cols_constant\n",
    "num_cols_mean\n",
    "num_cols_median\n",
    "\n",
    "# 5.7 Our datasets are:\n",
    "\n",
    "X_train[:2],X_test[:2],y_train[:2],y_test[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot encoding all categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.0 What does OneHotEncoder do?\n",
    "#     Demo with 'sparse = False'\n",
    "\n",
    "# 6.1 Instantiate onehot class\n",
    "#     through 'ohe' object\n",
    "\n",
    "ohe = onehot(sparse = False)\n",
    "\n",
    "# 6.2 Let 'ohe' learn relevant data\n",
    "#     properties. Our demo data is: 4 X 2\n",
    "\n",
    "ohe.fit(\n",
    "         [                            # A list of lists\n",
    "            ['big'   , 'yes' ],\n",
    "            ['small' , 'no'  ],\n",
    "            ['medium', 'yes' ],\n",
    "            ['big'   , 'no'  ]\n",
    "         ]\n",
    "       )\n",
    "\n",
    "# 6.3 Use 'ohe' to transform demo data\n",
    "#     to dummy values\n",
    "\n",
    "ohe.transform([['big', 'yes'], ['small', 'no'], ['medium', 'yes'],['big','no']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.4 In future use 'ohe' to tansform any data with such levels.\n",
    "#     For example:\n",
    "\n",
    "ohe.transform(\n",
    "               [\n",
    "                   ['small'  , 'yes'],\n",
    "                   ['medium' , 'no' ]\n",
    "               ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.5 What does OneHotEncoder do?\n",
    "#    In one line, it transforms dense data to dummy values (1,0)\n",
    "#     Demo with 'sparse = True' -- Output is stored in a special compressed format\n",
    "#     See here dense matrix to sparse matrix conversion example:\n",
    "#     http://www.btechsmartclass.com/data_structures/ds_images/Triplet_Representation_of_Sparse_Matrix.png\n",
    "#     https://www.researchgate.net/publication/328995968/figure/fig4/AS:693582436528129@1542374347304/Illustration-of-the-sparse-matrix-format-A-Example-matrix-of-size-8-8-with-5.png\n",
    "\n",
    "# 6.5.1\n",
    "\n",
    "ohe = onehot(sparse = True)\n",
    "\n",
    "\n",
    "# 6.5.2\n",
    "sp = ohe.fit_transform([['big', 'yes'], ['small', 'no'], ['medium', 'yes'],['big','no']])\n",
    "\n",
    "# 6.5.3\n",
    "sp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.5.4  Transform sparse to dense form\n",
    "sp.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.6 One Hot Encode all categorical columns, cat_cols \n",
    "#     Note, by now all NaNs have been filled\n",
    "\n",
    "ohe = onehot(sparse = False)\n",
    "ohe.fit_transform(X_train_c[cat_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard scaling all numeric columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.7 Scale all numeric variables in the same manner\n",
    "#     Note, by now all NaNs have been dealth with\n",
    "ss = StandardScaler()\n",
    "ss.fit_transform(X_train_c[num_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenate pre-processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.0 So complete dataset is:\n",
    "a = ohe.transform(X_train_c[cat_cols])\n",
    "b = ss.transform(X_train_c[num_cols])\n",
    "\n",
    "# 7.1 Horizontally concatenate now\n",
    "Xtrain = np.hstack([a,b]) # It is not X_train\n",
    "Xtrain.shape # (8,14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label encoding target\n",
    "Code text values in target column to digits. Our <i>y_train</i> (<i>'Creditability'</i> column) has values 'yes', 'no'..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.0 Label encode target feature\n",
    "# 8.0.1 Our target\n",
    "f\"Unencoded target column is:\"\n",
    "y_train\n",
    "\n",
    "# 8.0.2 Encode now\n",
    "le = LabelEncoder()\n",
    "le.fit(y_train)\n",
    "\n",
    "y_train = le.fit_transform(y_train)\n",
    "\n",
    "# 8.0.3 Just print transformed y_train\n",
    "f\"Encoded y_train is: {y_train}\"   # Encoded y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision tree Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.0 Train model using Xtrain\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "\n",
    "# 9.1 Instantiate DecisionTreeClassifier class\n",
    "\n",
    "dt = DecisionTreeClassifier()\n",
    "\n",
    "# 9.2 Use the classifier object to train \n",
    "#     on our data\n",
    "\n",
    "dt.fit(Xtrain,y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform X_test_c\n",
    "Before making predictions, we need to tranform columns of <i>X_test_c</i> in the same manner we did to X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.3 We now want to predict values for X_test_c\n",
    "\n",
    "# 9.4 First, transform X_test_c in the same manner as we did for X_train\n",
    "#      But this time, there will be no 'fit()'. We will use\n",
    "#       already fitted objects for transformations.\n",
    "\n",
    "#     NOTE: If you have find the error of there being new levels in Xtest,\n",
    "#           execute all code quickly from #4.0 onwards.\n",
    "\n",
    "X_test_c[num_cols_median]  = si_median.transform(X_test_c[num_cols_median])\n",
    "X_test_c[num_cols_mean]    = si_mean.transform(X_test_c[num_cols_mean])\n",
    "X_test_c[cat_cols_constant]= si_constant.transform(X_test_c[cat_cols_constant])\n",
    "X_test_c[cat_cols_mf]      = si_mf.transform(X_test_c[cat_cols_mf]) \n",
    "a                          = ohe.transform(X_test_c[cat_cols])\n",
    "b                          = ss.transform(X_test_c[num_cols])\n",
    "Xtest = np.hstack([a,b])   # Final transformed X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.5 Also label encode, y_test\n",
    "#     Using earlier fitted 'le' object\n",
    "f\"Values in y_test are:\"\n",
    "y_test\n",
    "\n",
    "y_test = le.transform(y_test)\n",
    "f\"Transformed y_test is: {y_test}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.6 Make prediction for Xtest\n",
    "dt.predict(Xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II\n",
    "## Data preprocessing with Pipelining\n",
    "Pre-process each one of the four subsets of data though a pipe and also perform modeling in pipe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10.0 Create pipes for all transformations\n",
    "# Ref: https://scikit-learn.org/stable/modules/compose.html#pipeline\n",
    "\n",
    "#     ColumnTransformer: Applies specified transformations parallely to each data-subset\n",
    "#     Pipeline         :  Applies transformation sequentially through transformers. \n",
    "#                         Input to pipe is one data-subset. Output of one transformer\n",
    "#                         is fed to another.\n",
    "\n",
    "#     Big picture\n",
    "#     i)   top_pipeline = Pipeline(ColumnTranformer, Estimator)\n",
    "#     ii)  ColumnTransformer([(pipe_mean, cols_mean),(pipe_median, cols_median),\n",
    "#                             (pipe_mf,   cols_mf),  (pipe_constant,cols_const])\n",
    "#     iii) pipe_mean = Pipeline([(imputer), (StandardScaler)]) \n",
    "#      iv) pipe_median=....\n",
    "#\n",
    "#     The pipeline can be used as any other estimator\n",
    "#     and avoids leaking the test set into the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10.1 Instantiate Pipeline object for processing numerical data. Impute = mean\n",
    "#     Pipeline as a composite transformer\n",
    "\n",
    "pipe_mean_transformer = Pipeline(\n",
    "                                  [\n",
    "                                    ('si', SimpleImputer(strategy='mean')),\n",
    "                                    ('ss1', StandardScaler())\n",
    "                                  ]\n",
    "                                 )\n",
    "\n",
    "# 10.1.1 Train pipe, just to test if it works\n",
    "pipe_mean_transformer.fit_transform(X_train[num_cols_mean])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10.2 Instantiate Pipeline object for processing numerical data. Impute = median\n",
    "pipe_median_transformer = Pipeline(\n",
    "                                     [\n",
    "                                        ('sm', SimpleImputer(strategy='median')),\n",
    "                                        ('ss2', StandardScaler())\n",
    "                                      ]\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10.3 Instantiate Pipeline object for processing cat data. Impute = most_frequent\n",
    "pipe_mf_transfomer = Pipeline(\n",
    "                                [\n",
    "                                  ('mf', SimpleImputer(strategy='most_frequent')),\n",
    "                                  ('ohe', onehot())\n",
    "                                ]\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10.4 Instantiate Pipeline object for processing cat data. Impute = constant\n",
    "sc = SimpleImputer(strategy=\"constant\", fill_value = 'missing')\n",
    "\n",
    "pipe_constant_transformer = Pipeline(\n",
    "                                       [\n",
    "                                          ('cons', sc),\n",
    "                                          ('ohe', onehot())\n",
    "                                        ]\n",
    "                                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10.5 Collecting all pipes in column transformer\n",
    "#     along with column names\n",
    "#                       some-name  transformer     col-names\n",
    "ct_transformer = ColumnTransformer(\n",
    "                                    [\n",
    "                                       ('pm',    pipe_mean_transformer    ,     num_cols_mean   ),\n",
    "                                       ('pme',   pipe_median_transformer  ,    num_cols_median  ),\n",
    "                                       ('pmf',   pipe_mf_transfomer       ,        cat_cols_mf  ),\n",
    "                                       ('pcons', pipe_constant_transformer,   cat_cols_constant )\n",
    "                                    ]\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10.6 Final pipeline for transformation and modeling\n",
    "#     final_pipe is both a \n",
    "final_pipe_transformer_estimator = Pipeline(\n",
    "                                             [\n",
    "                                                 ('ct', ct_transformer),            # Column transformer object\n",
    "                                                 ('dt', DecisionTreeClassifier()) # Estimator\n",
    "                                             ]\n",
    "                                           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train final_pipe on data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11.0 Train on data using final_pipe\n",
    "#     We use (X_train, y_train)\n",
    "\n",
    "final_pipe_transformer_estimator.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11.1 Make prediction on test data\n",
    "#     Note that there is no need to separately\n",
    "#     transform X_test. Pipes take care of that\n",
    "\n",
    "final_pipe_transformer_estimator.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11.2 But what is the actual y_test\n",
    "y_test\n",
    "# le.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## That's all folks ##########"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
